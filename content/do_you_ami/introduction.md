+++

title= "Introduction"
date= "2018-06-28T00:00:00Z"
weight = 10  # Order that this section will appear.

+++


Unfortunately nowadays more and more episodes of harassments against women arose and misogynistic comments can be found in social media, where misogynists hide themselves behind the security of the anonymity. Therefore, **it is very important to identify misogyny in social media**. Recent investigations studied how the misogyny phenomenon takes place, for example as unjustified slurring or as stereotyping of the role/body of a woman (i.e. the hashtag #getbacktokitchen), as described in the book by Poland[[1]](#ref1). 
A preliminary research work was conducted by Hewitt et al. [[2]](#ref2) as first attempt of manually classification of misogynous tweets. 
Automatic misogyny identification in Social Media has been firstly investigated by Anzovino et al. [[3]](#ref3). 


However, when training a supervised model, **it is important to guarantee the fairness of the model** and therefore to reduce the error due to **unintended bias** [[4]](#ref4), i.e. the attitude of a  model to perform better on comments about some groups than for comments about others groups. As shown in [[5]](#ref5), when addressing misogyny detection problems, this biased behaviour of a model on new posts can be observed when processing sentences containing specific *identity terms* that likely conveyed misogyny in the training data, e.g. ''girlfriend'' and ''wife''.


The shared task has been organized in occasion of [IberEval-2018](https://sites.google.com/view/ibereval-2018)[[6]](#ref6), [Evalita 2018](http://www.evalita.it/2018)[[7]](#ref7) and as a part of the HatEval shared task at [SemEval 2019](http://alt.qcri.org/semeval2019/)[[8]](#ref8).
